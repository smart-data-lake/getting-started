
// This is the CustomWebserviceDataObject implementation after completing part-3a

package com.sample

import com.sample.CustomWebserviceDataObject.{twoDaysSeconds, twoWeeksSeconds}
import com.typesafe.config.Config
import io.smartdatalake.config.SdlConfigObject.DataObjectId
import io.smartdatalake.config.{FromConfigFactory, InstanceRegistry}
import io.smartdatalake.util.hdfs.PartitionValues
import io.smartdatalake.util.misc.SmartDataLakeLogger
import io.smartdatalake.util.webservice.SttpUtil
import io.smartdatalake.workflow.dataframe.GenericSchema
import io.smartdatalake.workflow.dataframe.spark.SparkSchema
import io.smartdatalake.workflow.dataobject.{CanCreateSparkDataFrame, DataObject, DataObjectMetadata}
import io.smartdatalake.util.webservice.HttpTimeoutConfig
import io.smartdatalake.workflow.{ActionPipelineContext, ExecutionPhase}
import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.types.ArrayType
import org.json4s.{DefaultFormats, Formats}
import sttp.client3.{HttpClientSyncBackend, Identity, SttpBackend, SttpBackendOptions, basicRequest}
import sttp.model.Uri

import java.util.concurrent.TimeUnit
import scala.concurrent.duration.FiniteDuration

// Default to the interval of [2 weeks and 2 days ago] -> [2 weeks ago]
case class DepartureQueryParameters(airport: String, begin: Long = System.currentTimeMillis() / 1000 - twoWeeksSeconds - twoDaysSeconds, end: Long = System.currentTimeMillis() / 1000 - twoWeeksSeconds )

case class State(airport: String, nextBegin: Long)

/**
 * [[DataObject]] to call webservice and return response as a DataFrame
 */
case class CustomWebserviceDataObject(override val id: DataObjectId,
                                      responseRowSchema: GenericSchema,
                                      queryParameters: Seq[DepartureQueryParameters],
                                      timeouts: Option[HttpTimeoutConfig] = None,
                                      baseUrl: String,
                                      nRetry: Int = 1,
                                      mockJsonDataObject: Option[String] = None,
                                      override val metadata: Option[DataObjectMetadata] = None
                                     )
                                     (@transient implicit val instanceRegistry: InstanceRegistry)
  extends DataObject with CanCreateSparkDataFrame with SmartDataLakeLogger {

  val sparkResponseRowSchema = responseRowSchema.asInstanceOf[SparkSchema].inner

  @transient private lazy val httpBackend: SttpBackend[Identity, Any] = {
    val options = SttpBackendOptions.Default
    timeouts.map(_.connectionTimeoutMs).foreach(ms => options.connectionTimeout(FiniteDuration(ms, TimeUnit.MILLISECONDS)))
    HttpClientSyncBackend(options = options)
  }

  private def queryWebservice(url: String, retry: Int = nRetry): String = {

    var request = basicRequest
      .get(Uri.unsafeParse(url))
      .followRedirects(true)
    timeouts.map(_.readTimeoutMs).foreach(ms => request = request.readTimeout(FiniteDuration(ms, TimeUnit.MILLISECONDS)))

    val content = try {
      SttpUtil.getContent(request.send(httpBackend), "queryWebservice")
    } catch {
      case e: Exception =>
        if (retry == 0) {
          logger.error(e.getMessage, e)
          throw e
        }
        logger.info(s"($id) Request will be repeated, because the server responded with: ${e.getMessage}. \nRequest retries left: ${retry - 1}")
        queryWebservice(url, retry - 1)
    }
    logger.info(s"($id) Success for request ${url}")

    content
  }

  override def getSparkDataFrame(partitionValues: Seq[PartitionValues])(implicit context: ActionPipelineContext): DataFrame = {
    import org.apache.spark.sql.functions._
    implicit val formats: Formats = DefaultFormats
    val session = context.sparkSession
    import session.implicits._

    // if time interval is more than a week, set end config to 4 days after begin
    def checkQueryParameters(queryParameters: Seq[DepartureQueryParameters]) = {
      queryParameters.map {
        param =>
          val diff = param.end - param.begin
          if (diff / (3600 * 24) >= 7) {
            param.copy(end = param.begin + 3600 * 24 * 4)
          } else {
            param
          }
      }
    }

    if(context.phase == ExecutionPhase.Init){
      // simply return an empty data frame
      Seq[String]().toDF("response")
        .select(from_json($"response", ArrayType(sparkResponseRowSchema)).as("response"))
        .select(explode($"response").as("record"))
        .select("record.*")
        .withColumn("created_at", current_timestamp())
    } else {
      // use the queryParameters from the config
      val currentQueryParameters = checkQueryParameters(queryParameters)

      // given the query parameters, generate all requests
      val departureRequests = currentQueryParameters.map(
        param => s"${baseUrl}?airport=${param.airport}&begin=${param.begin}&end=${param.end}"
      )
      // make requests
      val departuresResponses = departureRequests.map { req =>
        logger.info(s"($id) Going to request: " + req)
        queryWebservice(req)
      }
      // create dataframe with the correct schema and add created_at column with the current timestamp
      val departuresDf = departuresResponses.toDF("response")
        .select(from_json($"response", ArrayType(sparkResponseRowSchema)).as("response"))
        .select(explode($"response").as("record"))
        .select("record.*")
        .withColumn("created_at", current_timestamp())

      // put simple nextState logic below

      // return
      departuresDf
    }

  }

  override def factory: FromConfigFactory[DataObject] = CustomWebserviceDataObject
}

object CustomWebserviceDataObject extends FromConfigFactory[DataObject] with SmartDataLakeLogger {
  override def fromConfig(config: Config)(implicit instanceRegistry: InstanceRegistry): CustomWebserviceDataObject = {
    extract[CustomWebserviceDataObject](config)
  }

  val twoWeeksSeconds = 60 * 60 * 24 * 14
  val twoDaysSeconds  = 60 * 60 * 24 * 2
}
