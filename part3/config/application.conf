
# This is the application.conf file as it should look after completing part-3

global {
  spark-options {
    "spark.sql.shuffle.partitions" = 2
    "spark.databricks.delta.snapshotPartitions" = 2
    # shared local metastore (metastore must be started)
    #"spark.hadoop.javax.jdo.option.ConnectionURL" = "jdbc:derby://localhost:1527/db;create=true"
    #"spark.hadoop.javax.jdo.option.ConnectionDriverName" = "org.apache.derby.jdbc.ClientDriver"
    #"spark.hadoop.javax.jdo.option.ConnectionUserName" = "sa"
    #"spark.hadoop.javax.jdo.option.ConnectionPassword" = "1234"
    # shared local s3 storage service for bucket 'data' (s3proxy must be started)
    #"spark.hadoop.fs.s3a.bucket.data.impl" = "org.apache.hadoop.fs.s3a.S3AFileSystem"
    #"spark.hadoop.fs.s3a.bucket.data.access.key" = "admin"
    #"spark.hadoop.fs.s3a.bucket.data.secret.key" = "1234"
    #"spark.hadoop.fs.s3a.bucket.data.endpoint" = "http://s3:8888/"
  }
  environment {
    hadoopFileStateStoreIndexAppend = true
  }
}

env {
  basePath = "/mnt/data/"
  #basePath = "s3a://data/"
}

dataObjects {
  ext-departures {
    type = CustomWebserviceDataObject
    responseRowSchema = "icao24 string, firstSeen integer, estDepartureAirport string, lastSeen integer, estArrivalAirport string, callsign string, estDepartureAirportHorizDistance integer, estDepartureAirportVertDistance integer, estArrivalAirportHorizDistance integer, estArrivalAirportVertDistance integer, departureAirportCandidatesCount integer, arrivalAirportCandidatesCount integer"
    baseUrl = "https://opensky-network.org/api/flights/departure"
    nRetry = 5
    queryParameters = [{
      airport = "LSZB"
    },{
      airport = "EDDF"
    }]
    timeouts {
      connectionTimeoutMs = 200000
      readTimeoutMs = 200000
    }
    metadata = {
          name = "Flight Departures Web Download"
          description = "OpenSky Network flight departures"
          layer = "extern"
          subjectArea = "flight data"
          tags = ["aviation", "flight", "departures"]
    }
    # enable to use mock-data as fallback if webservice is down
    #mockJsonDataObject = stg-departures-mock
  }
  # data object with mock data used by ext-departures, if webservice is not available
  stg-departures-mock {
    type = JsonFileDataObject
    path = ${env.basePath}fallback-download/stg-departures
    schema = ${dataObjects.ext-departures.responseRowSchema}
  }
  int-departures {
    type = DeltaLakeTableDataObject
    path = ${env.basePath}"~{id}"
    table {
      db = "default"
      name = "int_departures"
      primaryKey = [icao24, estdepartureairport, dt]
    }
    metadata = {
          name = "Flight Departures"
          description = "OpenSky Network flight departures"
          layer = "integration"
          subjectArea = "flight data"
          tags = ["aviation", "flight", "departures"]
    }
  }

  ext-airports {
    type = WebserviceFileDataObject
    # uses redirects to the URL below
    url = "https://ourairports.com/data/airports.csv"
    # url = "https://davidmegginson.github.io/ourairports-data/airports.csv"
    followRedirects = true
    readTimeoutMs=200000
  }
  stg-airports {
    type = CsvFileDataObject
    path = ${env.basePath}"~{id}"
    metadata {
              name = "Staging file of Airport location data"
              description = "contains beside GPS coordiantes, elevation, continent, country, region"
              layer = "staging"
              subjectArea = "airports"
              tags = ["aviation", "airport", "location"]
    }
  }
  int-airports {
    type = DeltaLakeTableDataObject
    path = ${env.basePath}"~{id}"
    table {
      db = "default"
      name = "int_airports"
      primaryKey = [ident]
    }
    metadata {
              name = "Airport locations"
              description = "airport names and locations"
              layer = "integration"
              subjectArea = "airports"
              tags = ["aviation", "airport", "location"]
    }
  }

  btl-departures-arrivals-airports {
    type = DeltaLakeTableDataObject
    path = ${env.basePath}"~{id}"
    table {
      db = "default"
      name = "btl_departures_arrivals_airports"
    }
  }

  btl-distances {
    type = DeltaLakeTableDataObject
    path = ${env.basePath}"~{id}"
    table {
      db = "default"
      name = "btl_distances"
    }
  }

}

actions {

  download-airports {
    type = FileTransferAction
    inputId = ext-airports
    outputId = stg-airports
    metadata {
             name = "Airport ingestion"
             description = "download airport data and write into CSV"
             tags = ["download", "websource"]
             feed = download
     }
  }

  download-deduplicate-departures {
    type = DeduplicateAction
    inputId = ext-departures
    outputId = int-departures
    executionMode = { type = DataObjectStateIncrementalMode }
    mergeModeEnable = true
    updateCapturedColumnOnlyWhenChanged = true
    transformers = [{
      type = SQLDfTransformer
      code = "select ext_departures.*, date_format(from_unixtime(firstseen),'yyyyMMdd') dt from ext_departures"
    },{
      type = ScalaCodeSparkDfTransformer
      code = """
        import org.apache.spark.sql.{DataFrame, SparkSession}
        def transform(session: SparkSession, options: Map[String,String], df: DataFrame, dataObjectId: String) : DataFrame = {
          import session.implicits._
          df.dropDuplicates("icao24", "estdepartureairport", "dt")
        }
        // return as function
        transform _
      """
    }]
    metadata {
              name = "Flight departure data injection"
              description = "download and cleaning of flight departure data"
              tags = ["deduplicated", "DeltaLake"]
              feed = compute
    }
  }

  historize-airports {
    type = HistorizeAction
    inputId = stg-airports
    outputId = int-airports
    transformers = [{
      type = SQLDfTransformer
      code = "select ident, name, latitude_deg, longitude_deg from stg_airports"
    }]
     metadata {
          name = "Airport historization and filter"
          description = "Filter name and coordinates of airports and hisorize data"
          tags = ["historize", "filter", "DeltaLake"]
          feed = compute
     }
  }

  join-departures-airports {
    type = CustomDataFrameAction
    inputIds = [int-departures, int-airports]
    outputIds = [btl-departures-arrivals-airports]
    transformers = [{
      type = SQLDfsTransformer
      code = {
        btl-connected-airports = """
          select int_departures.estdepartureairport, int_departures.estarrivalairport,
            airports.*
          from int_departures join int_airports airports on int_departures.estArrivalAirport = airports.ident
        """
      }},
      {
        type = SQLDfsTransformer
        code = {
          btl-departures-arrivals-airports = """
            select btl_connected_airports.estdepartureairport, btl_connected_airports.estarrivalairport,
              btl_connected_airports.name as arr_name, btl_connected_airports.latitude_deg as arr_latitude_deg, btl_connected_airports.longitude_deg as arr_longitude_deg,
              airports.name as dep_name, airports.latitude_deg as dep_latitude_deg, airports.longitude_deg as dep_longitude_deg
            from btl_connected_airports join int_airports airports on btl_connected_airports.estdepartureairport = airports.ident
          """
        }
      }
    ]
    metadata {
          name = "Airport Departures Join"
          description = "merging flight details and airport locations"
          tags = ["merge", "airports", "coordinates"]
          feed = compute
    }
  }

  compute-distances {
    type = CopyAction
    inputId = btl-departures-arrivals-airports
    outputId = btl-distances
    transformers = [{
      type = ScalaClassSparkDfTransformer
      className = com.sample.ComputeDistanceTransformer
    }]
    metadata {
              name = "Flight distance computation"
              description = "compute the flight distance between departure and arrival"
              tags = ["compute", "coordinates", "distances"]
              feed = compute
    }
  }
}
